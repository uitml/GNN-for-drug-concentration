{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The commands install the required libraries:\n",
    "!pip install pytorch\n",
    "!pip install pytorch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a3b51",
   "metadata": {},
   "source": [
    "# SMILES_paper_LOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f9705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bf6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1c7bd",
   "metadata": {},
   "source": [
    "Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57de5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES_list=['CN1[C@H]2CC[C@@H]1[C@H]([C@H](C2)OC(=O)C3=CC=CC=C3)C(=O)OC', 'CCC(=O)C1(CCN(CC1)C)C2=CC(=CC=C2)O', 'CCC(=O)C(CC(C)N(C)C)(C1=CC=CC=C1)C2=CC=CC=C2', 'COC(=O)C(C1CCCCN1)C2=CC=CC=C2', 'CN(C)C[C@H]1CCCC[C@@]1(C2=CC(=CC=C2)O)O', 'CN(C)C[C@H]1CCCC[C@@]1(C2=CC(=CC=C2)OC)O', 'CC1=CC=C(C=C1)C2=C(N3C=C(C=CC3=N2)C)CC(=O)N(C)C', 'CN1C(=O)CN=C(C2=C1C=CC(=C2)N)C3=CC=CC=C3F', 'CN1C2CCC1C(C(C2)OC(=O)C3=CC=CC=C3)C(=O)O', 'CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)OC)O[C@H]3[C@H](C=C4)O', 'CN[C@@]1(CCCCC1=O)C2=CC=CC=C2Cl', 'CC(CC1=CC2=C(C=C1)OCO2)NC', 'CNC(C)Cc1ccccc1', 'CC(=O)O[C@H]1C=C[C@H]2[C@H]3CC4=C5[C@]2([C@H]1OC5=C(C=C4)O)CCN3C', 'C1C(=O)NC2=C(C=C(C=C2)N)C(=N1)C3=CC=CC=C3', 'CN=C1CN(C(=C2C=C(C=CC2=N1)Cl)C3=CC=CC=C3)O', 'CN1C(=O)CN=C(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3', 'CN1C(=O)CN=C(C2=C1C=CC(=C2)[N+](=O)[O-])C3=CC=CC=C3F', 'CN1CC[C@]23[C@@H]4C(=O)CC[C@]2([C@H]1CC5=C3C(=C(C=C5)OC)O4)O', 'C1C(=O)NC2=C(C=C(C=C2)N)C(=N1)C3=CC=CC=C3Cl', 'CC1=NN=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4', 'CC(CC1=CC=CC=C1)N', 'CC(C(=O)C1=CC=CC=C1)N', 'CC(N)Cc1ccc2c(c1)OCO2', 'CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)O)O[C@H]3[C@H](C=C4)O', 'C1C(=O)NC2=C(C=C(C=C2)Cl)C(=N1)C3=CC=CC=C3', 'CN1C2=C(C=C(C=C2)Cl)C(=NC(C1=O)O)C3=CC=CC=C3', 'C1C(=O)NC2=C(C=C(C=C2)[N+](=O)[O-])C(=N1)C3=CC=CC=C3Cl', 'C1C(=O)NC2=C(C=C(C=C2)[N+](=O)[O-])C(=N1)C3=CC=CC=C3', 'C1=CC=C(C=C1)C2=NC(C(=O)NC3=C2C=C(C=C3)Cl)O', 'C1C(=O)NC2=C(C=C(C=C2)Br)C(=N1)C3=CC=CC=N3', 'C[C@@H]([C@H](C1=CC=CC=C1)O)N', 'CC1=C(C(=O)N2CC(CCC2=N1)O)CCN3CCC(CC3)C4=NOC5=C4C=CC(=C5)F', 'CN(C)CCC=C1C2=CC=CC=C2CCC3=CC=CC=C31', 'C1CC(=O)NC2=C1C=CC(=C2)OCCCCN3CCN(CC3)C4=C(C(=CC=C4)Cl)Cl', 'CN1[C@@H]2CC[C@H]1CC(C2)OC(=O)C(CO)C3=CC=CC=C3', 'C1CN(CCN1CCOCC(=O)O)C(C2=CC=CC=C2)C3=CC=C(C=C3)Cl', 'CN(C)CCC=C1C2=CC=CC=C2SC3=C1C=C(C=C3)Cl', 'CN(C)CCCC1(C2=C(CO1)C=C(C=C2)C#N)C3=CC=C(C=C3)F', 'CN1CCN(CC1)C2=NC3=C(C=CC(=C3)Cl)NC4=CC=CC=C42', 'CCC(=O)N(C1CCN(CC1)CCC2=CC=CC=C2)C3=CC=CC=C3', 'C1CN(CCC1(C2=CC=C(C=C2)Cl)O)CCCC(=O)C3=CC=C(C=C3)F', 'C1CN(CCN1CCOCCO)C(C2=CC=CC=C2)C3=CC=C(C=C3)Cl', 'CCN(CC)CC(=O)NC1=C(C=CC=C1C)C', 'CCN(CC)CCNC(=O)C1=CC(=C(C=C1OC)N)Cl', 'CC(C)NCC(COC1=CC=C(C=C1)CCOC)O', 'CN1CCN2C(C1)C3=CC=CC=C3CC4=CC=CC=C42', 'CC1=NC=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4F', 'CN1CCN2C(C1)C3=CC=CC=C3CC4=C2N=CC=C4', 'CNCCCC1(C2=C(CO1)C=C(C=C2)C#N)C3=CC=C(C=C3)F', 'Clc1ccc2c(c1)N=C(N1CCNCC1)c1ccccc1N2', 'CNCCC=C1C2=CC=CC=C2CCC3=CC=CC=C31', 'CN(C)CC(C1=CC=C(C=C1)O)C2(CCCCC2)O', 'CC(=O)NC1=CC=C(C=C1)O', 'CC(CN1C2=CC=CC=C2SC3=CC=CC=C31)N(C)C', 'CC(C)NCC(COC1=CC=CC2=CC=CC=C21)O', 'C1CN(CCN1CCOCCO)C2=NC3=CC=CC=C3SC4=CC=CC=C42', 'CC1=C(C(=O)N2CCCCC2=N1)CCN3CCC(CC3)C4C5=C(C=C(C=C5)F)ON4', 'CN(C)CC(C1=CC=C(C=C1)OC)C2(CCCCC2)O', 'CCOC(=O)C1=C(COCCN)NC(=C(C1c2ccccc2Cl)C(=O)OC)C', 'C[C@@H]([C@@H](C1=CC=CC=C1)O)NC', 'CC1=CC2=C(S1)NC3=CC=CC=C3N=C2N4CCN(CC4)C', 'CCC1=CC=CC=C1NC(=O)C(CC2=C(C=C(C=C2)OC)OC)C#N', 'C1C(=O)N=C2C=CC(=CC2=C(N1O)C3=CC=CC=C3)Cl', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O', 'CN(C)CCCN1C2=CC=CC=C2CCC3=CC=CC=C31', 'CN(C)CCCN1C2=CC=CC=C2CCC3=CC=CC=C31', 'CCN1CCCC1CNC(=O)C2=CC(=C(C=C2OC)N)S(=O)(=O)CC', 'C1CN(C[C@H]([C@@H]1C2=CC=C(C=C2)F)COC3=CC4=C(C=C3)OCO4)C=O', 'CCN(CC)CCN1C2=C(C=C(C=C2)[N+](=O)[O-])N=C1CC3=CC=C(C=C3)OC(C)C', 'C[C@]([C@H]1C[C@@]23CC[C@@]1([C@H]4[C@@]25CCN([C@@H]3CC6=C5C(=C(C=C6)O)O4)CC7CC7)OC)(C(C)(C)C)O', 'C1=CC=C(C(=C1)C2=NC(C(=O)NC3=C2C=C(C=C3)Cl)O)Cl', 'CCN(CC)CCN1C(=O)CN=C(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3F', 'C1C(=S)N(C2=C(C=C(C=C2)Cl)C(=N1)C3=CC=CC=C3F)CC(F)(F)F', 'CC1=NN(C2=C1C(=NCC(=O)N2C)C3=CC=CC=C3F)C', 'C1C(=O)N(C2=C(C=C(C=C2)Cl)C(=N1)C3=CC=CC=C3)CC(F)(F)F', 'C1C(=O)NC2=C(C=C(C=C2)Br)C(=N1)C3=CC=CC=C3Cl', 'CN1C2=C(C=C(C=C2)Cl)C(=NC(C1=O)O)C3=CC=CC=C3Cl', 'C1CCC2=C(C1)C3=C(S2)NC(=O)CN=C3C4=CC=CC=C4', 'CCC1=CC2=C(S1)N3C(=NN=C3CN=C2C4=CC=CC=C4)C', 'CCC1=CC2=C(S1)N3C(=NN=C3CN=C2C4=CC=CC=C4Cl)C', 'CN1C(=O)CN=C(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3Cl', 'CN1C(=O)CN=C(C2=C1C=CC(=C2)[N+](=O)[O-])C3=CC=CC=C3', 'CC1=NN=C2N1C3=C(C=C(C=C3)Br)C(=NC2)C4=CC=CC=C4F', 'C[C@H]1C(=O)NC2=C(C=C(C=C2)[N+](=O)[O-])C(=N1)C3=CC=CC=C3Cl', 'CC1=NN=C2N1C3=C(C=C(C=C3)Br)C(=NC2)C4=CC=CC=N4', 'C1C(=O)NC2=C(C=C(C=C2)Br)C(=N1)C3=CC=CC=C3F', 'CCC(=O)N(C1=CC=CC=C1)C2(CCN(CC2)CCN3C(=O)N(N=N3)CC)COC', 'CCCCC1=C(C2=CC=CC=C2O1)C(=O)C3=CC(=C(C(=C3)I)OCCN(CC)CC)I', 'CC(C)NCC(COC1=CC=C(C=C1)CC(=O)N)O', 'CC(C)NCC(COC1=CC=C(C=C1)COCCOC(C)C)O', 'C1=CC=C(C=C1)CCCCOCCCCCCNCC(C2=CC(=C(C=C2)O)CO)O', 'C1CN(CCC1C2=CN(C3=C2C=C(C=C3)Cl)C4=CC=C(C=C4)F)CCN5CCNC5=O', 'CC(C)C(CCCN(C)CCC1=CC(=C(C=C1)OC)OC)(C#N)C2=CC(=C(C=C2)OC)OC', 'CC1=CC(=C(C=C1)SC2=CC=CC=C2N3CCNCC3)C', 'CC(=O)CC(C1=CC=CC=C1)C2=C(C3=CC=CC=C3OC2=O)O', 'CC1=CC(=CC(=C1CC2=NCCN2)C)C(C)(C)C', 'C1CN(CCN1CCC2=C(C=C3C(=C2)CC(=O)N3)Cl)C4=NSC5=CC=CC=C54', 'C1CC2=C(C=CC(=C2)Cl)C(=C3CCNCC3)C4=C1C=CC=N4', 'C1CN=C(N1)NC2=C(C=CC=C2Cl)Cl', 'C1CN(CCN1CCC=C2C3=CC=CC=C3SC4=C2C=C(C=C4)C(F)(F)F)CCO', 'CCC1(C(=O)NCNC1=O)C2=CC=CC=C2', 'CC(=O)NCCC1=CC=CC2=C1C=C(C=C2)OC', 'CC1=CC=CC=C1OC(CCNC)C2=CC=CC=C2', 'O=C(C1N(CCCC1)CCCC)NC2=C(C)C=CC=C2C', 'COC1=C(C=C2C(=C1)CC(C2=O)CC3CCN(CC3)CC4=CC=CC=C4)OC', 'COC1=C(C=C2C(=C1)C(=NC(=N2)N3CCN(CC3)C(=O)C4COC5=CC=CC=C5O4)N)OC', 'CNCCC(C1=CC=CC=C1)OC2=CC=C(C=C2)C(F)(F)F', 'CC(=O)NC(COC)C(=O)NCC1=CC=CC=C1', 'CC1CCN(CC1)CCCC(=O)C2=CC=C(C=C2)F', 'C1COCCN1CCNC(=O)C2=CC=C(C=C2)Cl', 'CN1CCC2=CC3=C(C(=C2[C@@H]1[C@@H]4C5=C(C(=C(C=C5)OC)OC)C(=O)O4)OC)OCO3', 'O=C(Nc1c(cccc1C)C)[C@H]2N(CCC)CCCC2', 'CNS(=O)(=O)CC1=CC2=C(C=C1)NC=C2CCN(C)C', 'CN(C)CCC1=CNC2=C1C=C(C=C2)C[C@H]3COC(=O)N3', 'CC(C(=O)C1=CC(=CC=C1)Cl)NC(C)(C)C', 'CNCCC(C1=CC=CS1)OC2=CC=CC3=CC=CC=C32', 'CN1CC[C@]23[C@@H]4[C@H]1CC5=C2C(=C(C=C5)O)O[C@H]3C(=O)CC4', 'C1CC1CN2CCC34C5C(=O)CCC3(C2CC6=C4C(=C(C=C6)O)O5)O', 'CN1CCN(CC1)C(=O)O[C@H]2C3=NC=CN=C3C(=O)N2C4=NC=C(C=C4)Cl', 'CN(C)CCC1=CNC2=C1C=C(C=C2)CS(=O)(=O)N3CCCC3', 'CN1CC2C(C1)C3=C(C=CC(=C3)Cl)OC4=CC=CC=C24', 'CC(C)/C=C/CCCCC(=O)NCC1=CC(=C(C=C1)O)OC', 'CN(C)CC/C=C/1\\C2=CC=CC=C2CSC3=CC=CC=C31', 'CC1=CC(=CC=C1)CN2CCN(CC2)C(C3=CC=CC=C3)C4=CC=C(C=C4)Cl', 'CC1=C(C(=CC=C1)C)NC(=O)C2CCCCN2C', 'CCOC(=O)C1(CCN(CC1)C)C2=CC=CC=C2', 'C1CCN(CC1)C2(CCN(CC2)CCCC(=O)C3=CC=C(C=C3)F)C(=O)N', 'CCC(=O)N(C1=CC=CC=C1)C2(CCN(CC2)CCC(=O)OC)C(=O)OC', 'C1=CC(=C(C(=C1)F)CN2C=C(N=N2)C(=O)N)F', 'CC(C)NCC(C1=CC=C(C=C1)NS(=O)(=O)C)O', 'CCOC(=O)[C@H](CCC1=CC=CC=C1)N[C@@H](C)C(=O)N2[C@H]3CCCC[C@@H]3C[C@H]2C(=O)O', 'C1CN(CCN1CCCN2C(=O)N3C=CC=CC3=N2)C4=CC(=CC=C4)Cl', 'C1CN(CCN1CCC=C2C3=CC=CC=C3SC4=C2C=C(C=C4)Cl)CCO', 'CN1C(=O)CC(=O)N(C2=C1C=CC(=C2)Cl)C3=CC=CC=C3', 'CC1=NN=C2N1C3=C(C=C(C=C3)Cl)C(=NC2)C4=CC=CC=C4Cl', 'CC12CC3CC(C1)(CC(C3)(C2)N)C', 'CCCCC1=NC(=C(N1CC2=CC=C(C=C2)C3=CC=CC=C3C4=NNN=N4)CO)Cl', 'CCCN[C@H]1CCC2=C(C1)SC(=N2)N', 'COC1=CC=CC=C1CNCCC2=CC(=C(C=C2OC)Br)OC', 'C1CCC(CC1)(C2=CC(=CC=C2)O)N3CCCCC3', 'CCC1=CC=C(C=C1)C(=O)C(C)NC', 'CC1=CC=CC=C1C(C2=CC=CC=C2)OCCN(C)C', 'CCCN(CCC)CCC1=C2CC(=O)NC2=CC=C1', 'CN(C)CCc1c[nH]c2ccc(O)cc12', 'CN(C)CCOC(C1=CC=CC=C1)C2=CC=CC=C2', 'CC(=O)NCCC1=CNC2=C1C=C(C=C2)OC', 'CC1=CC(=C(C=C1OC)CCN)OC', 'CCCC1O[C@@H]2C[C@H]3[C@@H]4CCC5=CC(=O)C=C[C@@]5([C@H]4[C@H](C[C@@]3([C@@]2(O1)C(=O)CO)C)O)C', 'CC(CC1=CC(=CC=C1)F)N', 'C[C@]12CC[C@H]3[C@H]([C@@H]1C[C@@H]([C@]2(C)O)O)CC[C@@H]4[C@@]3(CC5=C(C4)NN=C5)C', 'CCN(CC)C(=O)[C@H]1CN([C@@H]2CC3(C4=C(C2=C1)C=CC=C4NC3=O)O)C', 'O=C(O)C1=CN(CCCCCF)C2=C1C=CC=C2', 'CC1C(C(CC(O1)OC2C(OC(CC2O)OC3C(OC(CC3O)OC4CCC5(C(C4)CCC6C5CC(C7(C6(CCC7C8=CC(=O)OC8)O)C)O)C)C)C)O)O', 'COC1=CC=CC=C1CNCCC2=CC(=C(C=C2OC)Cl)OC', 'CC(C)N(CCC1=CNC2=C1C(=CC=C2)OC(=O)C)C(C)C', 'S2c1ccccc1N(c3c2cccc3)CC(C)CN(C)C', 'CC(C(=O)C1=CC(=CC=C1)Cl)NC(C)(C)CO', 'CCSC1=C(C=C(C(=C1)OC)CCN)OC', 'CC(CC1=CC2=C(C=C1)OC=C2)N', 'CC(=O)NC1=CC2=C(C=C1)NC(=O)CN=C2C3=CC=CC=C3Cl', 'CC(C)C(C(=O)N)NC(=O)C1=NN(C2=CC=CC=C21)CC3=CC=C(C=C3)F', 'c1ccc(C)cc1CC(C)N', 'CC(N)Cc1ccc(Cl)cc1', 'CC12C=CC3=C4CCC(=O)C=C4CCC3C1CCC2O', 'CN1C(=O)CC(=O)N(C2=C1C=CC(=C2)Cl)C3=CC=C(C=C3)O', 'C1CN(CCN1CCOCCO)C2=NC3=C(C=C(C=C3)O)SC4=CC=CC=C42', 'CC(CC1=CC2=C(C=C1)NC=C2)N', 'COC1=CC=CC=C1CNCCC2=CC(=C(C=C2OC)I)OC', 'CC(NC)C(C1=CC=C(C)C=C1)=O', 'CC(C)(C)NCC(C1=CC(=CC(=C1)OC(=O)N(C)C)OC(=O)N(C)C)O', 'COc1cc(CCN)c(OC)cc1I', 'CCN(CC)C(C)C(=O)C1=CC=CC=C1', 'CC(C)C(C(=O)N)NC(=O)C1=NN(C2=CC=CC=C21)CC3=CC=C(C=C3)F', 'CC(CC1=CC=CC=C1)N', 'CC(N)CC1=CC=C(F)C=C1', 'CC(CC1=CC2=C(C=C1)C=CO2)N', 'NC(CC1=CNC2=C1C=CC=C2)C', 'C1C2=NN=C(N2C3=C(C=C(C=C3)Cl)C(=N1)C4=CC=CC=C4)CO', 'C1=CC(=CC=C1C(CC(=O)O)CN)Cl', 'CCCC(C(=O)C1=CC=CC=C1)N2CCCC2', 'C1CC[C@]2([C@H]3CC4=C([C@]2(C1)CCN3CC5CCC5)C=C(C=C4)O)O', 'CC(C)(C)[C@@H](C(=O)N[C@@H](CC1=CC=CC=C1)[C@H](CN(CC2=CC=C(C=C2)C3=CC=CC=N3)NC(=O)[C@H](C(C)(C)C)NC(=O)OC)O)NC(=O)OC', 'CCC(C(=O)C1=CC=CC=C1)NC', 'C1(CN2CCNCC2)=CC=CC=C1', 'CC(C)SC1=C(C=C(C(=C1)OC)CCN)OC', 'Fc1ccc(cc1)CC(NC)C', ' NC(CC1=CC(OC)=C(Br)C=C1OC)C', 'CCCC(C)(COC(=O)N)COC(=O)NC(C)C', 'CC12CCC3C(C1CCC2O)CCC4=CC(=O)C=CC34C', 'CC12CCC3C(C1CCC2(C)O)CC(C4=CC(=O)C=CC34C)O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8aec6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[5.5281,5.3278,6.5455,6.9831,5.1069,5.2645,5.7044,4.91,4.5196,4.4913,4.6978,4.8265,4.2742,4.3,3.8435,3.77,4.2901,3.9187,4.8197,3.2277,5.1068,3.3635,2.5235,2.7164,2.2345,2.7047,2.7369,2.7811,2.6881,1.5956,1.0706,0.507,5.6363,5.6828,4.7587,5.4284,4.0292,5.2731,5.9359,5.3983,5.6898,5.577,5.4741,5.2224,5.1878,5.4584,5.648,5.2618,5.3835,5.8209,5.0553,5.7253,5.3098,2.4968,5.379,5.5683,5.7513,5.6628,5.5612,4.0242,4.2952,4.7429,4.1163,2.1344,-1.3782,5.8852,5.7991,5.6559,5.2036,4.5309,5.2359,1.6514,5.2821,4.5667,5.4035,4.1506,1.872,1.9964,5.1802,5.2846,5.0538,4.2904,4.2599,3.6107,3.0513,3.498,1.8218,5.6632,4.589,4.2214,5.3504,4.9334,5.0099,5.8504,4.7656,3.8934,6.0285,4.7487,4.5015,4.8084,4.8602,-0.229,4.7231,5.1908,5.9172,6.091,5.6285,5.4179,4.4855,5.5694,5.1098,5.8003,5.6169,5.2521,5.2817,4.5071,2.8847,3.8355,3.9521,3.4444,5.2173,5.126,4.0492,5.596,5.0528,5.1838,5.5187,5.291,5.3131,4.4156,4.3067,4.4902,5.3895,4.9663,3.471,3.8365,3.4336,3.2286,0.3632,5.1945,4.836,5.0706,4.8439,5.4786,4.4201,4.9847,4.5715,4.0931,3.7538,2.4844,4.6948,3.7484,0.9585,3.8653,5.5647,5.5177,5.373,4.5364,3.4746,2.739,2.461,2.4366,2.3989,2.1171,3.7769,1.9675,3.8601,1.1752,5.4739,4.9772,5.1621,3.5389,4.3054,3.0777,2.8528,2.0028,2.6467,1.7765,0.6149,0.0996,5.1298,5.6968,4.8862,4.4037,4.4362,3.5976,4.298,3.3625,4.1516,3.2779,-0.137]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ccd3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a5da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34.0, 34.0, 34.0, 28.0, 35.0, 35.0, 29.0, 30.0, 34.0, 34.0, 33.0, 35.0, 34.0, 27.0, 27.0, 26.0, 30.0, 30.0, 34.0, 27.0, 27.0, 35.0, 35.0, 34.0, 35.0, 27.0, 30.0, 27.0, 27.0, 30.0, 27.0, 34.0, 28.0, 34.0, 34.0, 34.0, 33.0, 35.0, 35.0, 35.0, 34.0, 34.0, 34.0, 34.0, 32.0, 35.0, 34.0, 27.0, 34.0, 34.0, 26.0, 35.0, 35.0, 28.0, 34.0, 35.0, 34.0, 28.0, 35.0, 33.0, 35.0, 26.0, 34.0, 28.0, 35.0, 35.0, 35.0, 34.0, 33.0, 34.0, 36.0, 30.0, 33.0, 33.0, 27.0, 28.0, 27.0, 30.0, 34.0, 34.0, 34.0, 30.0, 30.0, 27.0, 33.0, 27.0, 27.0, 34.0, 34.0, 35.0, 35.0, 30.0, 33.0, 36.0, 29.0, 27.0, 30.0, 33.0, 31.0, 36.0, 33.0, 33.0, 27.0, 29.0, 33.0, 28.0, 27.0, 34.0, 27.0, 35.0, 32.0, 34.0, 32.0, 38.0, 34.0, 33.0, 35.0, 34.0, 60.0, 35.0, 35.0, 34.0, 35.0, 34.0, 29.0, 28.0, 34.0, 34.0, 33.0, 30.0, 34.0, 34.0, 33.0, 33.0, 30.0, 27.0, 35.0, 34.0, 34.0, 27.0, 34.0, 34.0, 30.0, 34.0, 35.0, 35.0, 27.0, 29.0, 34.0, 35.0, 35.0, 34.0, 32.0, 35.0, 27.0, 35.0, 27.0, 35.0, 38.0, 35.0, 27.0, 35.0, 30.0, 34.0, 36.0, 30.0, 34.0, 34.0, 27.0, 33.0, 36.0, 28.0, 33.0, 35.0, 35.0, 34.0, 35.0, 36.0, 26.0, 30.0, 35.0, 34.0, 36.0, 34.0, 34.0, 35.0, 31.0, 36.0, 35.0, 35.0, 36.0]\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "\n",
    "# Function that computes an approximate 3D orientation angle for a molecule\n",
    "def calculate_orientation_angle(smiles):\n",
    "\n",
    "    # Convert SMILES string into an RDKit molecule\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Add hydrogens and generate 3D coordinates\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol, randomSeed=42)     # 3D embedding\n",
    "    AllChem.UFFOptimizeMolecule(mol)              # geometry optimization\n",
    "\n",
    "    # Extract atom coordinates from the optimized 3D conformer\n",
    "    conf = mol.GetConformer()\n",
    "    coords = conf.GetPositions()\n",
    "\n",
    "    # Create two vectors from the first atom to atoms 2 and 3\n",
    "    v1 = coords[1] - coords[0]\n",
    "    v2 = coords[2] - coords[0]\n",
    "\n",
    "    # Compute cosine of angle between the vectors\n",
    "    cos_angle = (\n",
    "        np.dot(v1, v2) /\n",
    "        (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    )\n",
    "\n",
    "    # Convert from radians to degrees\n",
    "    angle_rad = abs(np.arccos(cos_angle))\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    return angle_deg\n",
    "\n",
    "\n",
    "# Compute the orientation angle for each molecule in SMILES_list\n",
    "angles = []\n",
    "for smiles in SMILES_list:\n",
    "    angle = calculate_orientation_angle(smiles)\n",
    "    angle = np.around(angle)   # round angle to nearest degree\n",
    "    angles.append(angle)\n",
    "\n",
    "# Print the list of computed angles\n",
    "print(angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357af239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 2.0, 3.0, 1.0, 5.0, 3.0, 3.0, 3.0, 4.0, 4.0, 3.0, 3.0, 1.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 5.0, 2.0, 4.0, 4.0, 5.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 4.0, 4.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 4.0, 4.0, 3.0, 6.0, 2.0, 3.0, 4.0, 3.0, 3.0, 2.0, 2.0, 5.0, 5.0, 5.0, 5.0, 3.0, 4.0, 4.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 4.0, 4.0, 5.0, 6.0, 4.0, 6.0, 3.0, 4.0, 2.0, 4.0, 2.0, 2.0, 4.0, 3.0, 3.0, 3.0, 3.0, 4.0, 8.0, 4.0, 4.0, 2.0, 3.0, 7.0, 3.0, 3.0, 4.0, 2.0, 3.0, 4.0, 4.0, 5.0, 3.0, 2.0, 4.0, 2.0, 3.0, 3.0, 3.0, 4.0, 5.0, 3.0, 3.0, 6.0, 4.0, 3.0, 3.0, 3.0, 2.0, 4.0, 2.0, 4.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 2.0, 4.0, 4.0, 3.0, 11.0, 4.0, 3.0, 2.0, 3.0, 3.0, 2.0, 4.0, 5.0, 2.0, 1.0, 2.0, 4.0, 5.0, 2.0, 4.0, 2.0, 6.0, 3.0, 2.0, 5.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 10.0, 2.0, 2.0, 3.0, 2.0, 3.0, 5.0, 3.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdPartialCharges\n",
    "\n",
    "# Function that estimates a molecule's dipole moment using Gasteiger partial charges.\n",
    "# The value returned is a simple sum of the absolute Gasteiger charges of all atoms\n",
    "# (not a true physical dipole moment, but a charge-based descriptor).\n",
    "def calculate_dipole_moment(smiles):\n",
    "    \n",
    "    # Convert SMILES string into an RDKit molecule\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Add hydrogens because Gasteiger charges rely on a complete valence structure\n",
    "    mol = Chem.AddHs(mol)\n",
    "\n",
    "    # Generate and optimize a 3D molecular structure\n",
    "    AllChem.EmbedMolecule(mol)          # builds a 3D conformation\n",
    "    AllChem.MMFFOptimizeMolecule(mol)   # geometry optimization with MMFF94\n",
    "\n",
    "    # Compute Gasteiger partial charges for all atoms\n",
    "    rdPartialCharges.ComputeGasteigerCharges(mol)\n",
    "\n",
    "    # Approximate \"dipole moment\" as the sum of absolute Gasteiger charges\n",
    "    dipole_moment = sum(\n",
    "        abs(float(atom.GetProp('_GasteigerCharge')))\n",
    "        for atom in mol.GetAtoms()\n",
    "    )\n",
    "\n",
    "    return dipole_moment\n",
    "\n",
    "# Compute the dipole descriptor for each molecule in the SMILES list\n",
    "dipole_momentum = []\n",
    "for smiles in SMILES_list:\n",
    "    dipole_moment = calculate_dipole_moment(smiles)\n",
    "    dipole_moment = np.around(dipole_moment)  # round the value\n",
    "    dipole_momentum.append(dipole_moment)\n",
    "\n",
    "# Print the resulting dipole values\n",
    "print(dipole_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71be04",
   "metadata": {},
   "source": [
    "# For running code import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Deep Learning (PyTorch + PyTorch Geometric)\n",
    "# ----------------------------\n",
    "import torch                       # core PyTorch tensor & deep-learning tools\n",
    "import torch.nn as nn              # neural network layers\n",
    "import torch.nn.functional as F    # activation functions, loss functions\n",
    "from torch_geometric.data import Data, DataLoader     # graph data structures & loaders\n",
    "from torch_geometric.nn import GATConv                # Graph Attention Network layer\n",
    "from torch_geometric.utils import add_self_loops      # utility for adding self-connections\n",
    "\n",
    "# ----------------------------\n",
    "# Data Handling & Utilities\n",
    "# ----------------------------\n",
    "import pandas as pd                # tabular data manipulation\n",
    "from sklearn.model_selection import train_test_split  # train/test splitting\n",
    "\n",
    "# ----------------------------\n",
    "# RDKit (Cheminformatics)\n",
    "# ----------------------------\n",
    "from rdkit import Chem                             # molecule creation and SMILES handling\n",
    "from rdkit.Chem import AllChem, Descriptors        # 3D embedding, optimization, descriptors\n",
    "from rdkit.Chem.rdMolDescriptors import CalcMolFormula  # molecular formula calculator\n",
    "from rdkit.Chem import Draw                         # molecule visualization\n",
    "from rdkit.Chem import rdMolHash                    # molecular hashing features\n",
    "from rdkit import DataStructs                       # similarity & fingerprint utilities\n",
    "\n",
    "# ----------------------------\n",
    "# Similarity & Math Tools\n",
    "# ----------------------------\n",
    "from sklearn.metrics.pairwise import cosine_similarity   # cosine similarity for vector sets\n",
    "from collections import Counter, defaultdict             # counting & key–value grouping tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090887bb",
   "metadata": {},
   "source": [
    "# Features and edge indexes for the GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2280c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vsa(smiles):\n",
    "    # Convert the SMILES string into an RDKit molecule\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # If SMILES cannot be parsed, return None\n",
    "    if mol is None:\n",
    "        print(\"Invalid SMILES!\")\n",
    "        return None\n",
    "    \n",
    "    # Add hydrogens to complete the valence structure\n",
    "    mol_with_hydrogens = Chem.AddHs(mol)\n",
    "    \n",
    "    # Generate a 3D conformation for the molecule\n",
    "    AllChem.EmbedMolecule(mol_with_hydrogens)\n",
    "    \n",
    "    # Compute the molecular volume (an RDKit VSA/3D volume estimate)\n",
    "    volume = AllChem.ComputeMolVolume(mol_with_hydrogens)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "#run 2 times (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118e7ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Compute molecular volumes for all SMILES strings in the list\n",
    "volumes = []\n",
    "for el_smiles in SMILES_list:\n",
    "    \n",
    "    # Calculate volume using the previously defined VSA/3D volume function\n",
    "    volume = calculate_vsa(el_smiles)\n",
    "    \n",
    "    # Append only valid results (skip invalid SMILES)\n",
    "    if volume is not None:\n",
    "        volumes.append(volume)\n",
    "\n",
    "# Print how many volumes were successfully computed\n",
    "print(len(volumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e40d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Function to calculate the 3D dimensions (width, length, height) of a molecule\n",
    "def calculate_molecule_dimensions(smiles):\n",
    "    try:\n",
    "        # Convert SMILES string to an RDKit molecule\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # Add explicit hydrogens to complete valence\n",
    "        mol = Chem.AddHs(mol)\n",
    "\n",
    "        # Generate and optimize a 3D conformation\n",
    "        AllChem.EmbedMolecule(mol)\n",
    "        AllChem.MMFFOptimizeMolecule(mol)\n",
    "        \n",
    "        # Extract 3D coordinates of all atoms\n",
    "        coords = mol.GetConformer().GetPositions()\n",
    "        \n",
    "        # Compute maximum dimensions along x, y, and z axes\n",
    "        width  = max(coord[0] for coord in coords) - min(coord[0] for coord in coords)\n",
    "        length = max(coord[1] for coord in coords) - min(coord[1] for coord in coords)\n",
    "        height = max(coord[2] for coord in coords) - min(coord[2] for coord in coords)\n",
    "\n",
    "        # Return dimensions as a dictionary\n",
    "        return {'Width': width, 'Length': length, 'Height': height}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle errors (e.g., invalid SMILES) gracefully\n",
    "        print(f\"Error processing molecule: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fd13532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Compute the widths (x-axis dimension) of all molecules in SMILES_list\n",
    "widths = []\n",
    "\n",
    "for smiles in SMILES_list:\n",
    "    # Calculate 3D dimensions of the molecule\n",
    "    dimensions = calculate_molecule_dimensions(smiles)\n",
    "    \n",
    "    # Extract the width (x-axis) and append to the list\n",
    "    width = dimensions['Width']\n",
    "    widths.append(width)    \n",
    "\n",
    "# Print how many widths were successfully calculated\n",
    "print(len(widths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc2277c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Compute the lengths (y-axis dimension) of all molecules in SMILES_list\n",
    "lengths = []\n",
    "\n",
    "for smiles in SMILES_list:\n",
    "    # Calculate 3D dimensions of the molecule\n",
    "    dimensions = calculate_molecule_dimensions(smiles)\n",
    "    \n",
    "    # Extract the length (y-axis) and append to the list\n",
    "    length = dimensions['Length']\n",
    "    lengths.append(length)    \n",
    "\n",
    "# Print how many lengths were successfully calculated\n",
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9cdf247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Compute the heights (z-axis dimension) of all molecules in SMILES_list\n",
    "heights = []\n",
    "\n",
    "for smiles in SMILES_list:\n",
    "    # Calculate 3D dimensions of the molecule\n",
    "    dimensions = calculate_molecule_dimensions(smiles)\n",
    "    \n",
    "    # Extract the height (z-axis) and append to the list\n",
    "    height = dimensions['Height']\n",
    "    heights.append(height)    \n",
    "\n",
    "# Print how many heights were successfully calculated\n",
    "print(len(heights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc21d40",
   "metadata": {},
   "source": [
    "# Features extration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store atomic feature vectors for all molecules\n",
    "x = []\n",
    "\n",
    "# Index to iterate through SMILES_list\n",
    "di = 0\n",
    "\n",
    "while di < len(SMILES_list):\n",
    "    # Get SMILES string for the current molecule\n",
    "    smiles = SMILES_list[di]\n",
    "\n",
    "    # Convert SMILES to an RDKit molecule object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize a list to store features for each atom in the molecule\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each atom in the molecule\n",
    "    for atom in mol.GetAtoms():\n",
    "        # Get the element symbol (e.g., 'C', 'O', 'N')\n",
    "        element = atom.GetSymbol()\n",
    "\n",
    "        # Define atomic features as integers (1/0 for booleans)\n",
    "        features = [\n",
    "            int(atom.IsInRing()),              # 1 if atom is in a ring, else 0\n",
    "            int(atom.GetIsAromatic()),         # 1 if atom is aromatic, else 0\n",
    "            int(atom.GetDegree()),             # Number of directly bonded neighbors\n",
    "            int(atom.GetAtomicNum()),          # Atomic number\n",
    "            int(atom.GetTotalNumHs()),         # Total number of hydrogens\n",
    "            int(atom.GetTotalValence()),       # Total valence electrons\n",
    "            int(atom.GetNumRadicalElectrons()),# Number of radical electrons\n",
    "            int(atom.GetFormalCharge()),       # Formal charge\n",
    "            int(atom.GetHybridization()),      # Hybridization type as integer\n",
    "            # Additional features derived from atomic properties\n",
    "            int((atom.GetMass() - 10.812)/116.092),\n",
    "            int((atom.GetAtomicNum() - 1.5)/0.6),\n",
    "            int((atom.GetAtomicNum() - 0.64)/0.76)\n",
    "        ]\n",
    "\n",
    "        # Append molecular-level features for this atom\n",
    "        # e.g., volumes, widths, lengths, heights (computed previously)\n",
    "        for additional_list in [volumes, widths, lengths, heights]:\n",
    "            additional_feature = additional_list[di]\n",
    "            features.append(additional_feature)\n",
    "\n",
    "        # Add the atom's feature vector to the molecule's list\n",
    "        result.append(features)\n",
    "\n",
    "    # Add the molecule's atom features to the overall list\n",
    "    x.append(result)\n",
    "\n",
    "    # Move to the next molecule\n",
    "    di += 1\n",
    "\n",
    "# Print the complete list of atomic feature vectors for all molecules\n",
    "print(\"x=\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd208f73",
   "metadata": {},
   "source": [
    "Edge indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store edge indices for all molecules\n",
    "edge_index = []\n",
    "\n",
    "# Iterate over each molecule in the SMILES list\n",
    "for smiles in SMILES_list:\n",
    "    # Convert SMILES string to an RDKit molecule object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize a list to store the edges for this molecule\n",
    "    edge_list_node = []\n",
    "\n",
    "    # Iterate over all bonds in the molecule\n",
    "    for bond in mol.GetBonds():\n",
    "        # Get the indices of the two atoms connected by the bond\n",
    "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        \n",
    "        # Add the bond as an edge (start → end) to the molecule's edge list\n",
    "        edge_list_node.append([start, end])\n",
    "\n",
    "    # Append this molecule's edge list to the overall edge_index list\n",
    "    edge_index.append(edge_list_node)\n",
    "\n",
    "# Print the list of edge indices for all molecules\n",
    "print(\"edge_index =\", edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcc40f",
   "metadata": {},
   "source": [
    "Similarity molecules' structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e754bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# Initialize a list to store graph objects for all molecules\n",
    "data_list = []\n",
    "\n",
    "# Iterate over all molecules to create PyTorch Geometric Data objects\n",
    "for ij in range(len(x)):\n",
    "    # Convert atom feature list for this molecule to a PyTorch tensor\n",
    "    x_graph = torch.tensor(x[ij], dtype=torch.float32)\n",
    "    \n",
    "    # Convert edge list for this molecule to a PyTorch tensor\n",
    "    # Transpose and make contiguous to match PyTorch Geometric edge_index format [2, num_edges]\n",
    "    edge_index_graph = torch.tensor(edge_index[ij], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Convert target property/label for this molecule to a tensor\n",
    "    y_graph = torch.tensor(y[ij], dtype=torch.float32)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object for this molecule\n",
    "    data_graph = Data(x=x_graph, edge_index=edge_index_graph, y=y_graph)\n",
    "    \n",
    "    # Add the graph to the list of all graph objects\n",
    "    data_list.append(data_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4371d2",
   "metadata": {},
   "source": [
    "# The GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adbcc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# PyTorch core\n",
    "# ----------------------------\n",
    "import torch                       # core PyTorch library for tensors and computation\n",
    "import torch.nn as nn              # neural network layers (Linear, Conv, etc.)\n",
    "import torch.nn.functional as F    # activation functions, loss functions, and other utilities\n",
    "\n",
    "# ----------------------------\n",
    "# PyTorch Geometric (Graph Neural Networks)\n",
    "# ----------------------------\n",
    "from torch_geometric.data import Data, DataLoader  # graph data structures and batching utilities\n",
    "from torch_geometric.nn import GATConv             # Graph Attention Network layer\n",
    "from torch_geometric.utils import add_self_loops   # utility to add self-loops to edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06ced891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "# Graph Neural Network model using Graph Attention Network (GAT) layers\n",
    "class GSR(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GSR, self).__init__()\n",
    "        \n",
    "        # First GAT layer: input features -> hidden features\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        \n",
    "        # Second GAT layer: hidden features -> hidden features\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Optional extra layer for deeper architectures for log_IE library: https://link.springer.com/article/10.1007/S00216-022-04084-6\n",
    "        # self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Third GAT layer: hidden features -> hidden features\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Fourth GAT layer: hidden features -> output features\n",
    "        self.conv4 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Extract node features and edge indices from the input graph\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Add self-loops to ensure each node attends to itself\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Apply first GAT layer and tanh activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        # Apply second GAT layer and tanh activation\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        # Optional 5th layer (commented out)\n",
    "        # x = self.conv5(x, edge_index)\n",
    "        # x = F.tanh(x) \n",
    "        \n",
    "        # Apply third GAT layer (if conv5 not used) and tanh activation\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.tanh(x)   \n",
    "        \n",
    "        # Apply final GAT layer to produce output features (e.g., regression or classification targets)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89e4296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GSR (Graph Attention Network) model\n",
    "model = GSR(in_channels=16, hidden_channels=28, out_channels = 1)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.000075)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba195c0",
   "metadata": {},
   "source": [
    "# Training GNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802e68a",
   "metadata": {},
   "source": [
    "The model applies a Leave-One-Out Cross-Validation (LOOCV) framework designed for very small molecular datasets, treating each molecule as the validation target while the others serve as training examples. Although the training loop is configured for up to 8000–10,000 epochs, only the first few epochs involve meaningful gradient updates; the remaining epochs function as repeated inference steps that monitor prediction stability, capture output degeneracy, and identify consistent prediction regions. At each epoch, the model records rounded predictions for the left-out molecule and tracks how often identical values appear, providing a signal of model certainty and robustness—especially valuable when structurally similar molecules are scarce. This long evaluation schedule effectively acts as a temporal ensemble, improving reliability for rare or unrepresented structural classes. For every LOOCV fold, the code generates a detailed summary containing per-epoch predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8366b06",
   "metadata": {},
   "source": [
    "# LOOCV + early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Training and evaluation parameters\n",
    "# ----------------------------\n",
    "early_stopping_epochs_list = [8000]    # Epochs at which to stop early for validation\n",
    "max_total_epochs = 8001             # Maximum total training epochs\n",
    "#For LOD and log_IE libraries, it's better to train the model during 10K epochs (for saving computation time, max_total_epochs = 6K)\n",
    "#*TRICK 1: Solution estimate with early stoping but treck last node of the molecule or the 1st node for another properties\n",
    "#!!! TRICK 2: Gradient accumalation in LOOCV computations for both libraries is requered only a couple of epochs (it's possible to interrupt computations but total number of epochs for training need to be set as 8K epochs(!))\n",
    "downloads_path = Path.home() / \"Desktop\" / \"LOOCV_Results\" # Path to save output summaries\n",
    "\n",
    "# ----------------------------\n",
    "# Leave-One-Out Cross-Validation (LOOCV)\n",
    "#For small libraries (https://thesesjournal.com/index.php/1/article/view/659/547), it's better to apply LOOCV instead of k-fold \n",
    "#For more accurate predictions during training, it is necessary to have 2–3 molecules with similar structures (e.g., double oxygen, benzene rings (consider 6&10 Carbons as different classes), or multiple nitrogen bonds). Type of halogens, however, are not critical. Nevertheless, our model remains stable even for non-represented classes, although predictions for these molecules may be within a nearby expected concentration ranges.\n",
    "# ----------------------------\n",
    "for dw in range(len(data_list)):\n",
    "    # Select one molecule as the validation set\n",
    "    val_data_graph = data_list[dw]\n",
    "\n",
    "    # Remaining molecules as training set\n",
    "    train_data = [data_list[i] for i in range(len(data_list)) if i != dw]\n",
    "\n",
    "    all_results = {}  # Dictionary to store results for each early stopping epoch\n",
    "\n",
    "    # Iterate over the list of early stopping epochs\n",
    "    for early_stop_epoch in early_stopping_epochs_list:\n",
    "        predicted_values = []  # Stores predicted values per epoch\n",
    "        set_elements = []      # Stores counts of repeated predicted values per epoch\n",
    "\n",
    "        # ----------------------------\n",
    "        # Training loop\n",
    "        # ----------------------------\n",
    "        for epoch in range(max_total_epochs):\n",
    "            # Stop training once we reach the early stopping epoch\n",
    "            if epoch >= early_stop_epoch:\n",
    "                break\n",
    "\n",
    "            model.train()  # Set model to training mode\n",
    "            for data_graph in train_data:\n",
    "                # Forward pass\n",
    "                output = model(data_graph)\n",
    "\n",
    "                # Compute loss between predicted and true values\n",
    "                loss = criterion(output, data_graph.y)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Validation and prediction\n",
    "        # ----------------------------\n",
    "            if epoch % 1 == 0:  # Evaluate every epoch\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    # Predict and round values to 3 decimals\n",
    "                    y_pred = np.around(model(val_data_graph).cpu().numpy(), decimals=3)\n",
    "                    y_pred_flat = y_pred.flatten().tolist()\n",
    "\n",
    "                    # Count repeated predicted values\n",
    "                    counts = defaultdict(int)\n",
    "                    for val in y_pred.flatten():\n",
    "                        counts[np.round(val, 3)] += 1\n",
    "                    repeated_counts = [count for count in counts.values() if count >= 2]\n",
    "\n",
    "                    # Store repeated counts and predictions for this evaluation step\n",
    "                    set_elements.append(repeated_counts if repeated_counts else [0])\n",
    "                    predicted_values.append(y_pred_flat)\n",
    "\n",
    "        # Store results for this early stopping epoch\n",
    "        all_results[early_stop_epoch] = {\n",
    "            \"set_elements\": set_elements,\n",
    "            \"predicted_values\": predicted_values\n",
    "        }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Retrieve SMILES and true values for this validation molecule\n",
    "    # ----------------------------\n",
    "    SMILES_list = val_data_graph.SMILES if hasattr(val_data_graph, 'SMILES') else \"SMILES not found\"\n",
    "    y_value = val_data_graph.y.cpu().numpy().tolist() if hasattr(val_data_graph, 'y') else \"y not found\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Save detailed summary to a text file\n",
    "    # ----------------------------\n",
    "    txt_filename = f\"element_{dw}_summary.txt\"\n",
    "    txt_path = downloads_path / txt_filename\n",
    "\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(f\"Results for element {dw} as validation set:\\n\\n\")\n",
    "\n",
    "        for epoch_stop in early_stopping_epochs_list:\n",
    "            f.write(f\"Early stopping at epoch {epoch_stop}:\\n\")\n",
    "            f.write(f\"Set elements: {all_results[epoch_stop]['set_elements']}\\n\")\n",
    "            f.write(f\"Predicted values (list per epoch, each with length={len(all_results[epoch_stop]['predicted_values'][0])}):\\n\")\n",
    "            for idx, preds in enumerate(all_results[epoch_stop]['predicted_values']):\n",
    "                f.write(f\" Epoch {idx*10}: {preds}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(f\"SMILES list: {SMILES_list}\\n\")\n",
    "        f.write(f\"y values: {y_value}\\n\")\n",
    "\n",
    "    print(f\"Saved detailed summary for element {dw} to {txt_path}\")\n",
    "\n",
    "print(\"All detailed summaries saved to Downloads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77229d51",
   "metadata": {},
   "source": [
    "# Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203759fa",
   "metadata": {},
   "source": [
    "The updated code implements gradient accumulation, which allows the model to simulate a larger batch size without exceeding GPU memory limits. In this version, the loss for each training sample is scaled by the accumulation factor, and loss.backward() is called on every mini-batch to accumulate gradients. The optimizer step (optimizer.step()) and gradient reset (optimizer.zero_grad()) are performed only after a specified number of mini-batches (accumulation_steps), effectively updating the model as if it had seen a larger batch. This approach ensures stable training while controlling memory usage and allows the model to benefit from gradient information over multiple small batches before each parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed999c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Training and evaluation parameters\n",
    "# -----------------------------------------------------------\n",
    "early_stopping_epochs_list = [8000]    # Epochs for early stopping\n",
    "max_total_epochs = 8001\n",
    "\n",
    "# Gradient Accumulation factor\n",
    "accumulation_steps = 8   # <<< YOU CAN CHANGE THIS to 2 or 15\n",
    "#Only couple epochs of training are enough to get the stable accurate result\n",
    "#Gradient accumulation provides 2 times higher results (for MSE scores) as boostrap and it's faster in computations\n",
    "\n",
    "downloads_path = Path.home() / \"Desktop\" / \"LOOCV_Results\" \n",
    "downloads_path.mkdir(exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Leave-One-Out Cross-Validation (LOOCV)\n",
    "# -----------------------------------------------------------\n",
    "for dw in range(len(data_list)):\n",
    "    \n",
    "    # Validation molecule\n",
    "    val_data_graph = data_list[dw]\n",
    "\n",
    "    # Training molecules\n",
    "    train_data = [data_list[i] for i in range(len(data_list)) if i != dw]\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Loop over early stopping settings\n",
    "    for early_stop_epoch in early_stopping_epochs_list:\n",
    "\n",
    "        predicted_values = []\n",
    "        set_elements = []\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Training Loop\n",
    "        # ---------------------------------------------------\n",
    "        for epoch in range(max_total_epochs):\n",
    "\n",
    "            # Stop training early\n",
    "            if epoch >= early_stop_epoch:\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()  # Zero before accumulation loop\n",
    "\n",
    "            for i, data_graph in enumerate(train_data):\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data_graph)\n",
    "\n",
    "                # Loss scaled for gradient accumulation\n",
    "                loss = criterion(output, data_graph.y) / accumulation_steps\n",
    "\n",
    "                # Backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply optimizer step only once per accumulation_steps\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            # ---------------------------\n",
    "            # Validation every epoch\n",
    "            # ---------------------------\n",
    "            if epoch % 1 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = np.around(model(val_data_graph).cpu().numpy(), 3)\n",
    "                    y_pred_flat = y_pred.flatten().tolist()\n",
    "\n",
    "                    # Count repeated predictions\n",
    "                    counts = defaultdict(int)\n",
    "                    for val in y_pred.flatten():\n",
    "                        counts[np.round(val, 3)] += 1\n",
    "\n",
    "                    repeated_counts = [count for count in counts.values() if count >= 2]\n",
    "\n",
    "                    set_elements.append(repeated_counts if repeated_counts else [0])\n",
    "                    predicted_values.append(y_pred_flat)\n",
    "\n",
    "        # Save results for this early stop epoch\n",
    "        all_results[early_stop_epoch] = {\n",
    "            \"set_elements\": set_elements,\n",
    "            \"predicted_values\": predicted_values\n",
    "        }\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Store SMILES + true y-values\n",
    "    # -----------------------------------------------------------\n",
    "    SMILES_list = val_data_graph.SMILES if hasattr(val_data_graph, 'SMILES') else \"SMILES not found\"\n",
    "    y_value = val_data_graph.y.cpu().numpy().tolist() if hasattr(val_data_graph, 'y') else \"y not found\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Save detailed report\n",
    "    # -----------------------------------------------------------\n",
    "    txt_filename = f\"element_{dw}_summary.txt\"\n",
    "    txt_path = downloads_path / txt_filename\n",
    "\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(f\"Results for element {dw} as validation set:\\n\\n\")\n",
    "\n",
    "        for epoch_stop in early_stopping_epochs_list:\n",
    "            f.write(f\"Early stopping at epoch {epoch_stop}:\\n\")\n",
    "            f.write(f\"Set elements: {all_results[epoch_stop]['set_elements']}\\n\")\n",
    "\n",
    "            preds_list = all_results[epoch_stop][\"predicted_values\"]\n",
    "            if len(preds_list) > 0:\n",
    "                length_each = len(preds_list[0])\n",
    "            else:\n",
    "                length_each = 0\n",
    "\n",
    "            f.write(f\"Predicted values (list per epoch, each length={length_each}):\\n\")\n",
    "\n",
    "            for idx, preds in enumerate(preds_list):\n",
    "                f.write(f\" Epoch {idx}: {preds}\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(f\"SMILES list: {SMILES_list}\\n\")\n",
    "        f.write(f\"y values: {y_value}\\n\")\n",
    "\n",
    "    print(f\"Saved detailed summary for element {dw} to {txt_path}\")\n",
    "\n",
    "print(\"All detailed summaries saved to Desktop/LOOCV_Results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87a85d",
   "metadata": {},
   "source": [
    "# Results processing\n",
    "\n",
    "LOOCV Metrics Aggregator and Stability Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a6498",
   "metadata": {},
   "source": [
    "This script processes LOOCV prediction outputs generated across multiple independent runs and folds, computes aggregated error metrics, and reports their variability before and after the removal of selected molecules. It reads prediction files produced for each fold, extracts per-molecule prediction sequences, and evaluates four complementary performance indicators: \n",
    "- First prediction, \n",
    "- Second prediction, \n",
    "- Last prediction, \n",
    "- Average prediction\n",
    "\n",
    "across all epochs.\n",
    "\n",
    "For each prediction type, the script calculates MAE, MSE, RMSE, and Accuracy (with tolerance defined as log(2)), and reports mean values as well as fold-to-fold standard deviations. The workflow supports exclusion of problematic indices to re-evaluate model robustness without rerunning training. A simple built-in table printer displays results in a clean, aligned console format. Overall, the script provides a complete reproducible pipeline for analyzing LOOCV prediction stability, quantifying run-level and fold-level variability, and determining how molecule removal affects global error statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e43c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# TABLE \n",
    "# ----------------------------------------------------\n",
    "def print_table(headers, rows):\n",
    "    col_widths = [max(len(str(x)) for x in col) for col in zip(headers, *rows)]\n",
    "\n",
    "    def fmt_row(row):\n",
    "        return \" | \".join(str(x).ljust(w) for x, w in zip(row, col_widths))\n",
    "\n",
    "    sep = \"-+-\".join('-' * w for w in col_widths)\n",
    "\n",
    "    print(fmt_row(headers))\n",
    "    print(sep)\n",
    "    for row in rows:\n",
    "        print(fmt_row(row))\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# USER PARAMETERS\n",
    "# Threshold defines accuracy tolerance; num_runs and file paths\n",
    "# specify how many LOOCV runs are evaluated and where outputs are stored.\n",
    "# excluded_indices allows removing problematic molecules post-analysis.\n",
    "# ----------------------------------------------------\n",
    "threshold = math.log(2)\n",
    "num_runs = 5\n",
    "base_results_path = Path.home() / \"Desktop\" / \"LOOCV_Results\"\n",
    "#remove non-representative molecules for improving accuracy of the prediction\n",
    "excluded_indices = {3, 15, 30}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# METRIC COMPUTATION\n",
    "# Computes MAE, MSE, RMSE, and accuracy using filtered predictions,\n",
    "# ensuring incomplete folds do not distort the metrics.\n",
    "# ----------------------------------------------------\n",
    "def compute_metrics(y_true_list, y_pred_list):\n",
    "    filtered = [(yt, yp) for yt, yp in zip(y_true_list, y_pred_list) if yp is not None]\n",
    "    if not filtered:\n",
    "        return (0, 0, 0, 0)\n",
    "    y_true_clean, y_pred_clean = zip(*filtered)\n",
    "    n = len(y_true_clean)\n",
    "    mae = sum(abs(yt - yp) for yt, yp in zip(y_true_clean, y_pred_clean)) / n\n",
    "    mse = sum((yt - yp) ** 2 for yt, yp in zip(y_true_clean, y_pred_clean)) / n\n",
    "    rmse = math.sqrt(mse)\n",
    "    accuracy = sum(1 for yt, yp in zip(y_true_clean, y_pred_clean) if abs(yt - yp) <= threshold) / n\n",
    "    return mae, mse, rmse, accuracy\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# AVERAGE METRIC AGGREGATOR\n",
    "# Computes mean and standard deviation for (MAE, MSE, RMSE, Accuracy)\n",
    "# across multiple folds or runs.\n",
    "# ----------------------------------------------------\n",
    "def average_metrics(metrics_list):\n",
    "    if not metrics_list:\n",
    "        return {\n",
    "            \"MAE_avg\": 0, \"MAE_dev\": 0,\n",
    "            \"MSE_avg\": 0, \"MSE_dev\": 0,\n",
    "            \"RMSE_avg\": 0, \"RMSE_dev\": 0,\n",
    "            \"Accuracy_avg\": 0, \"Accuracy_dev\": 0\n",
    "        }\n",
    "\n",
    "    maes = [m[0] for m in metrics_list]\n",
    "    mses = [m[1] for m in metrics_list]\n",
    "    rmses = [m[2] for m in metrics_list]\n",
    "    accs = [m[3] for m in metrics_list]\n",
    "\n",
    "    return {\n",
    "        \"MAE_avg\": np.mean(maes),\n",
    "        \"MAE_dev\": np.std(maes, ddof=1) if len(maes) > 1 else 0,\n",
    "        \"MSE_avg\": np.mean(mses),\n",
    "        \"MSE_dev\": np.std(mses, ddof=1) if len(mses) > 1 else 0,\n",
    "        \"RMSE_avg\": np.mean(rmses),\n",
    "        \"RMSE_dev\": np.std(rmses, ddof=1) if len(rmses) > 1 else 0,\n",
    "        \"Accuracy_avg\": np.mean(accs),\n",
    "        \"Accuracy_dev\": np.std(accs, ddof=1) if len(accs) > 1 else 0\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# DEV UTIL\n",
    "# Computes the average of standard deviations across folds.\n",
    "# Gives a single stability value per metric.\n",
    "# ----------------------------------------------------\n",
    "def average_dev_only(dev_list):\n",
    "    return np.mean(dev_list) if dev_list else 0\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MAIN EVALUATION LOOP\n",
    "# Iterates through all LOOCV runs & folds, reads prediction files,\n",
    "# extracts predictions, aligns them with true labels, and computes metrics.\n",
    "# ----------------------------------------------------\n",
    "def run_evaluation(exclude_set=set()):\n",
    "    all_metrics = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "    all_metrics_dev = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "    all_metrics_raw = []\n",
    "\n",
    "    for run_idx in range(num_runs):\n",
    "        run_path = base_results_path / f\"run_{run_idx}\"\n",
    "\n",
    "        # Skip runs that do not exist.\n",
    "        if not run_path.exists():\n",
    "            continue\n",
    "\n",
    "        fold_paths = [p for p in run_path.glob(\"fold_*\") if p.is_dir()]\n",
    "        run_metrics = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "        run_metrics_raw = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # FOLD PROCESSING\n",
    "        # Reads all prediction files inside each fold and extracts:\n",
    "        # first, second, last, and average predictions for each molecule.\n",
    "        # ----------------------------------------------------\n",
    "        for folder_path in fold_paths:\n",
    "            indices, first_values, second_values, last_values, avg_values = [], [], [], [], []\n",
    "            txt_files = [f for f in os.listdir(folder_path)\n",
    "                         if f.endswith(\".txt\") and f.startswith(\"element_\")]\n",
    "            file_tuples = []\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # Parse filenames to extract molecule index.\n",
    "            # Only includes indices not excluded by user.\n",
    "            # ----------------------------------------------------\n",
    "            for filename in txt_files:\n",
    "                match = re.search(r\"element_(\\d+)_epoch_\\d+\\.txt\", filename)\n",
    "                if match:\n",
    "                    index = int(match.group(1))\n",
    "                    if index in exclude_set:\n",
    "                        continue\n",
    "                    file_tuples.append((index, filename))\n",
    "\n",
    "            file_tuples.sort(key=lambda x: x[0])\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # Extract prediction lists from each file using regex.\n",
    "            # Build lists of first/second/last/average predictions.\n",
    "            # ----------------------------------------------------\n",
    "            for index, filename in file_tuples:\n",
    "                with open(folder_path / filename, \"r\") as f:\n",
    "                    content = f.read()\n",
    "                    match_values = re.search(r\"Predicted values.*:\\s*(\\[.*\\])\", content)\n",
    "                    if match_values:\n",
    "                        y_pred = ast.literal_eval(match_values.group(1))\n",
    "                        first_values.append(y_pred[0] if len(y_pred) > 0 else None)\n",
    "                        second_values.append(y_pred[1] if len(y_pred) > 1 else None)\n",
    "                        last_values.append(y_pred[-1] if len(y_pred) > 0 else None)\n",
    "                        avg_values.append(sum(y_pred) / len(y_pred) if len(y_pred) > 0 else None)\n",
    "                        indices.append(index)\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # Compute metrics only if at least one molecule was processed.\n",
    "            # Aligns predictions with corresponding y_true_full indices.\n",
    "            # ----------------------------------------------------\n",
    "            if indices:\n",
    "                y_true_selected = [y_true_full[i] for i in indices]\n",
    "                run_metrics[\"First\"].append(compute_metrics(y_true_selected, first_values))\n",
    "                run_metrics[\"Second\"].append(compute_metrics(y_true_selected, second_values))\n",
    "                run_metrics[\"Last\"].append(compute_metrics(y_true_selected, last_values))\n",
    "                run_metrics[\"Average\"].append(compute_metrics(y_true_selected, avg_values))\n",
    "\n",
    "                run_metrics_raw[\"First\"].append(list(zip(indices, first_values)))\n",
    "                run_metrics_raw[\"Second\"].append(list(zip(indices, second_values)))\n",
    "                run_metrics_raw[\"Last\"].append(list(zip(indices, last_values)))\n",
    "                run_metrics_raw[\"Average\"].append(list(zip(indices, avg_values)))\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Compute fold-to-fold deviations (stability) for each prediction type.\n",
    "        # ----------------------------------------------------\n",
    "        for key in run_metrics:\n",
    "            maes = [m[0] for m in run_metrics[key]]\n",
    "            mses = [m[1] for m in run_metrics[key]]\n",
    "            rmses = [m[2] for m in run_metrics[key]]\n",
    "            accs = [m[3] for m in run_metrics[key]]\n",
    "\n",
    "            all_metrics_dev[key].append((\n",
    "                np.std(maes, ddof=1) if len(maes) > 1 else 0,\n",
    "                np.std(mses, ddof=1) if len(mses) > 1 else 0,\n",
    "                np.std(rmses, ddof=1) if len(rmses) > 1 else 0,\n",
    "                np.std(accs, ddof=1) if len(accs) > 1 else 0\n",
    "            ))\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Aggregate fold metrics to obtain run-level metrics.\n",
    "        # ----------------------------------------------------\n",
    "        avg_metrics_run = {key: average_metrics(run_metrics[key]) for key in run_metrics}\n",
    "        for key in all_metrics:\n",
    "            all_metrics[key].append(avg_metrics_run[key])\n",
    "\n",
    "        all_metrics_raw.append(run_metrics_raw)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Compute final averages across all runs + deviations.\n",
    "    # ----------------------------------------------------\n",
    "    final_avg_metrics = {}\n",
    "    for key in all_metrics:\n",
    "        metrics_list = [(m[\"MAE_avg\"], m[\"MSE_avg\"], m[\"RMSE_avg\"], m[\"Accuracy_avg\"])\n",
    "                        for m in all_metrics[key]]\n",
    "\n",
    "        base_metrics = average_metrics(metrics_list)\n",
    "\n",
    "        devs = all_metrics_dev[key]\n",
    "        base_metrics[\"MAE_dev\"] = average_dev_only([d[0] for d in devs])\n",
    "        base_metrics[\"MSE_dev\"] = average_dev_only([d[1] for d in devs])\n",
    "        base_metrics[\"RMSE_dev\"] = average_dev_only([d[2] for d in devs])\n",
    "        base_metrics[\"Accuracy_dev\"] = average_dev_only([d[3] for d in devs])\n",
    "\n",
    "        final_avg_metrics[key] = base_metrics\n",
    "\n",
    "    return final_avg_metrics, all_metrics_raw\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# RECOMPUTE METRICS AFTER MOLECULE EXCLUSION\n",
    "# Allows recalculation without re-running training, making analysis faster.\n",
    "# ----------------------------------------------------\n",
    "def recompute_after_exclusion(all_metrics_raw, exclude_set):\n",
    "    new_all_metrics = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "    all_metrics_dev = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "\n",
    "    for run_data in all_metrics_raw:\n",
    "        run_metrics = {\"First\": [], \"Second\": [], \"Last\": [], \"Average\": []}\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Filter out excluded molecule indices and recompute metrics per fold.\n",
    "        # ----------------------------------------------------\n",
    "        for pred_type in run_data:\n",
    "            filtered_entries = []\n",
    "            for fold_data in run_data[pred_type]:\n",
    "                filtered = [(y_true_full[idx], yp)\n",
    "                            for idx, yp in fold_data if idx not in exclude_set]\n",
    "                if filtered:\n",
    "                    y_true_clean, y_pred_clean = zip(*filtered)\n",
    "                    filtered_entries.append(\n",
    "                        compute_metrics(list(y_true_clean), list(y_pred_clean))\n",
    "                    )\n",
    "            run_metrics[pred_type] = filtered_entries\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Compute new fold-to-fold deviations after exclusion.\n",
    "        # ----------------------------------------------------\n",
    "        for key in run_metrics:\n",
    "            maes = [m[0] for m in run_metrics[key]]\n",
    "            mses = [m[1] for m in run_metrics[key]]\n",
    "            rmses = [m[2] for m in run_metrics[key]]\n",
    "            accs = [m[3] for m in run_metrics[key]]\n",
    "\n",
    "            all_metrics_dev[key].append((\n",
    "                np.std(maes, ddof=1) if len(maes) > 1 else 0,\n",
    "                np.std(mses, ddof=1) if len(mses) > 1 else 0,\n",
    "                np.std(rmses, ddof=1) if len(rmses) > 1 else 0,\n",
    "                np.std(accs, ddof=1) if len(accs) > 1 else 0\n",
    "            ))\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Average across folds for the run.\n",
    "        # ----------------------------------------------------\n",
    "        avg_metrics_run = {key: average_metrics(run_metrics[key])\n",
    "                           for key in run_metrics}\n",
    "\n",
    "        for key in new_all_metrics:\n",
    "            new_all_metrics[key].append(avg_metrics_run[key])\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Final averaging across runs after exclusion.\n",
    "    # ----------------------------------------------------\n",
    "    final_avg_metrics = {}\n",
    "    for key in new_all_metrics:\n",
    "        metrics_list = [(m[\"MAE_avg\"], m[\"MSE_avg\"], m[\"RMSE_avg\"], m[\"Accuracy_avg\"])\n",
    "                        for m in new_all_metrics[key]]\n",
    "        base_metrics = average_metrics(metrics_list)\n",
    "\n",
    "        devs = all_metrics_dev[key]\n",
    "        base_metrics[\"MAE_dev\"] = average_dev_only([d[0] for d in devs])\n",
    "        base_metrics[\"MSE_dev\"] = average_dev_only([d[1] for d in devs])\n",
    "        base_metrics[\"RMSE_dev\"] = average_dev_only([d[2] for d in devs])\n",
    "        base_metrics[\"Accuracy_dev\"] = average_dev_only([d[3] for d in devs])\n",
    "\n",
    "        final_avg_metrics[key] = base_metrics\n",
    "\n",
    "    return final_avg_metrics\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# Runs analysis before & after molecule exclusion and prints tables.\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n=== Metrics BEFORE removing molecules ===\")\n",
    "final_metrics_before, all_metrics_raw = run_evaluation(exclude_set=set())\n",
    "\n",
    "table = []\n",
    "for key, metrics in final_metrics_before.items():\n",
    "    table.append([\n",
    "        key,\n",
    "        round(metrics[\"MAE_avg\"], 4),\n",
    "        round(metrics[\"MAE_dev\"], 4),\n",
    "        round(metrics[\"MSE_avg\"], 4),\n",
    "        round(metrics[\"MSE_dev\"], 4),\n",
    "        round(metrics[\"RMSE_avg\"], 4),\n",
    "        round(metrics[\"RMSE_dev\"], 4),\n",
    "        round(metrics[\"Accuracy_avg\"], 4),\n",
    "        round(metrics[\"Accuracy_dev\"], 4)\n",
    "    ])\n",
    "\n",
    "print(\"\\nFinal Metrics (All n samples):\")\n",
    "print_table(\n",
    "    [\"Prediction Type\", \"MAE Avg\", \"MAE Dev\",\n",
    "     \"MSE Avg\", \"MSE Dev\", \"RMSE Avg\", \"RMSE Dev\",\n",
    "     \"Accuracy Avg\", \"Accuracy Dev\"],\n",
    "    table\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# ANALYSIS AFTER REMOVING SELECTED MOLECULES\n",
    "# Provides a simple way to test dataset sensitivity & robustness.\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n=== Metrics AFTER removing molecules ===\")\n",
    "final_metrics_after = recompute_after_exclusion(all_metrics_raw, excluded_indices)\n",
    "\n",
    "table = []\n",
    "for key, metrics in final_metrics_after.items():\n",
    "    table.append([\n",
    "        key,\n",
    "        round(metrics[\"MAE_avg\"], 4),\n",
    "        round(metrics[\"MAE_dev\"], 4),\n",
    "        round(metrics[\"MSE_avg\"], 4),\n",
    "        round(metrics[\"MSE_dev\"], 4),\n",
    "        round(metrics[\"RMSE_avg\"], 4),\n",
    "        round(metrics[\"RMSE_dev\"], 4),\n",
    "        round(metrics[\"Accuracy_avg\"], 4),\n",
    "        round(metrics[\"Accuracy_dev\"], 4)\n",
    "    ])\n",
    "\n",
    "print(\"\\nFinal Metrics (n-3 samples after removal):\")\n",
    "print_table(\n",
    "    [\"Prediction Type\", \"MAE Avg\", \"MAE Dev\",\n",
    "     \"MSE Avg\", \"MSE Dev\", \"RMSE Avg\", \"RMSE Dev\",\n",
    "     \"Accuracy Avg\", \"Accuracy Dev\"],\n",
    "    table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1794e31",
   "metadata": {},
   "source": [
    "# Boostrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cea76",
   "metadata": {},
   "source": [
    "This code performs bootstrap-based model evaluation by repeatedly resampling the training dataset with replacement, training the model up to a defined early-stopping epoch, and generating predictions on out-of-bag (OOB) samples for each bootstrap iteration. For every bootstrap sample, it creates a dedicated results folder, selects bootstrap training data, identifies OOB validation data, and trains the model until the stopping criterion is met. During training, it periodically evaluates the model on OOB graphs, records rounded node-level predictions, and counts repeated predicted values to capture structural patterns. The script stores all predicted values, repetition counts, and corresponding element indices, then writes each OOB element’s results into separate text files labeled by element index and early-stopping epoch. This produces a structured, per-bootstrap archive of model behavior useful for uncertainty estimation and robustness analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# --- User parameters ---\n",
    "early_stopping_epochs_list = [8000]  # Early stopping epochs\n",
    "max_total_epochs = 150               # Max training epochs\n",
    "num_bootstrap_samples = 5            # Number of bootstrap resamples\n",
    "\n",
    "# Base save path\n",
    "base_results_path = Path.home() / \"Desktop\" / ...\n",
    "base_results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Bootstrap loop ---\n",
    "for b in range(num_bootstrap_samples):\n",
    "    # Create a folder for this bootstrap sample\n",
    "    bootstrap_path = base_results_path / f\"bootstrap_{b}\"\n",
    "    bootstrap_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Sample training data with replacement\n",
    "    train_indices = np.random.choice(len(data_list), size=len(data_list), replace=True)\n",
    "    train_data = [data_list[i] for i in train_indices]\n",
    "\n",
    "    # Out-of-bag (OOB) data for validation\n",
    "    oob_indices = list(set(range(len(data_list))) - set(train_indices))\n",
    "    val_data_list = [data_list[i] for i in oob_indices] if oob_indices else []\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for early_stop_epoch in early_stopping_epochs_list:\n",
    "        predicted_values = []\n",
    "        set_elements = []\n",
    "        predicted_indices = []\n",
    "\n",
    "        for epoch in range(max_total_epochs):\n",
    "            if epoch >= early_stop_epoch:\n",
    "                break\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            for data_graph in train_data:\n",
    "                output = model(data_graph)\n",
    "                loss = criterion(output, data_graph.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Collect predictions on OOB data\n",
    "            if val_data_list:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for idx, val_data_graph in zip(oob_indices, val_data_list):\n",
    "                        y_pred = np.around(model(val_data_graph).cpu().numpy(), decimals=3)\n",
    "                        y_pred_flat = y_pred.flatten().tolist()\n",
    "\n",
    "                        # Count repeated values (rounded)\n",
    "                        counts = defaultdict(int)\n",
    "                        for val in y_pred.flatten():\n",
    "                            counts[np.round(val, 3)] += 1\n",
    "                        repeated_counts = [count for count in counts.values() if count >= 2]\n",
    "\n",
    "                        set_elements.append(repeated_counts if repeated_counts else [0])\n",
    "                        predicted_values.append(y_pred_flat)\n",
    "                        predicted_indices.append(idx)  # store the predicted element index\n",
    "\n",
    "        # Store results\n",
    "        all_results[early_stop_epoch] = {\n",
    "            \"set_elements\": set_elements,\n",
    "            \"predicted_values\": predicted_values,\n",
    "            \"predicted_indices\": predicted_indices\n",
    "        }\n",
    "\n",
    "    # --- Save each OOB element in its own txt file ---\n",
    "    for early_stop_epoch in early_stopping_epochs_list:\n",
    "        for idx, element_idx in enumerate(all_results[early_stop_epoch]['predicted_indices']):\n",
    "            txt_filename = f\"element_{element_idx}_epoch_{early_stop_epoch}.txt\"\n",
    "            txt_path = bootstrap_path / txt_filename\n",
    "\n",
    "            with open(txt_path, \"w\") as f:\n",
    "                f.write(f\"Bootstrap sample {b} - OOB element {element_idx}\\n\")\n",
    "                f.write(f\"Early stopping at epoch {early_stop_epoch}\\n\")\n",
    "                f.write(f\"Set elements (repeated counts): {all_results[early_stop_epoch]['set_elements'][idx]}\\n\")\n",
    "                f.write(f\"Predicted values (raw, pre-pooling, one per node): {all_results[early_stop_epoch]['predicted_values'][idx]}\\n\")\n",
    "\n",
    "    print(f\"✅ Saved bootstrap {b} results in separate files in folder {bootstrap_path}\")\n",
    "\n",
    "print(\"✅ All bootstrap summaries saved as individual txt files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e6047",
   "metadata": {},
   "source": [
    "This code automates the evaluation of prediction results stored across multiple folders by extracting numerical outputs from text files, aligning them with corresponding ground-truth values, filtering out unwanted indices, and computing error metrics for different types of predictions (first value, second value, last value, and average of each prediction list). It reads all files matching a specific naming pattern, parses the predicted arrays, and calculates MAE, MSE, RMSE, and an accuracy measure based on a threshold. After computing metrics for each folder, it aggregates the results, computes averages and standard deviations across all folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import math\n",
    "import statistics\n",
    "from tabulate import tabulate  # For pretty tables\n",
    "\n",
    "# List of folders to process\n",
    "folders = [\n",
    "    \"C:\\\\Users\\\\Desktop\\\\...\",\n",
    "    \"C:\\\\Users\\\\Desktop\\\\...\",\n",
    "    \"C:\\\\Users\\\\Desktop\\\\...\",\n",
    "    \"C:\\\\Users\\\\Desktop\\\\...\",\n",
    "    \"C:\\\\Users\\\\Desktop\\\\...\"\n",
    "]   # replace with your folder paths\n",
    "\n",
    "threshold = math.log(2)  # tolerance for accuracy\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true_list, y_pred_list):\n",
    "    # Remove None values\n",
    "    filtered = [(yt, yp) for yt, yp in zip(y_true_list, y_pred_list) if yp is not None]\n",
    "    if not filtered:\n",
    "        return (0, 0, 0, 0)\n",
    "    y_true_clean, y_pred_clean = zip(*filtered)\n",
    "    n = len(y_true_clean)\n",
    "    mae = sum(abs(yt - yp) for yt, yp in zip(y_true_clean, y_pred_clean)) / n\n",
    "    mse = sum((yt - yp) ** 2 for yt, yp in zip(y_true_clean, y_pred_clean)) / n\n",
    "    rmse = math.sqrt(mse)\n",
    "    accuracy = sum(1 for yt, yp in zip(y_true_clean, y_pred_clean) if abs(yt - yp) <= threshold) / n\n",
    "    return mae, mse, rmse, accuracy\n",
    "\n",
    "# Function to average metrics across folders\n",
    "def average_metrics(metrics_list):\n",
    "    maes = [m[0] for m in metrics_list]\n",
    "    mses = [m[1] for m in metrics_list]\n",
    "    rmses = [m[2] for m in metrics_list]\n",
    "    accs = [m[3] for m in metrics_list]\n",
    "\n",
    "    return {\n",
    "        \"MAE_avg\": sum(maes)/len(maes),\n",
    "        \"MAE_dev\": statistics.stdev(maes) if len(maes) > 1 else 0,\n",
    "        \"MSE_avg\": sum(mses)/len(mses),\n",
    "        \"RMSE_avg\": sum(rmses)/len(rmses),\n",
    "        \"Accuracy_avg\": sum(accs)/len(accs)\n",
    "    }\n",
    "\n",
    "# Containers for metrics\n",
    "all_metrics = {\n",
    "    \"First\": [],\n",
    "    \"Second\": [],\n",
    "    \"Last\": [],\n",
    "    \"Average\": []\n",
    "}\n",
    "\n",
    "# Example y_true_full; replace with your actual ground truth values\n",
    "y_true_full = y_true\n",
    "\n",
    "# Process each folder\n",
    "for folder_path in folders:\n",
    "    indices = []\n",
    "    first_values = []\n",
    "    second_values = []\n",
    "    last_values = []\n",
    "    avg_values = []\n",
    "\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\") and f.startswith(\"element_\")]\n",
    "\n",
    "    # Extract indices and sort files\n",
    "    file_tuples = []\n",
    "    for filename in txt_files:\n",
    "        match = re.search(r\"element_(\\d+)_epoch_\\d+\\.txt\", filename)\n",
    "        if match:\n",
    "            index = int(match.group(1))\n",
    "            file_tuples.append((index, filename))\n",
    "    file_tuples.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Read files and extract predictions\n",
    "    for index, filename in file_tuples:\n",
    "        indices.append(index)\n",
    "        with open(os.path.join(folder_path, filename), \"r\") as f:\n",
    "            content = f.read()\n",
    "        match_values = re.search(r\"Predicted values.*:\\s*(\\[.*\\])\", content)\n",
    "        if match_values:\n",
    "            values_str = match_values.group(1)\n",
    "            y_pred = ast.literal_eval(values_str)\n",
    "            first_values.append(y_pred[0] if len(y_pred) > 0 else None)\n",
    "            second_values.append(y_pred[1] if len(y_pred) > 1 else None)\n",
    "            last_values.append(y_pred[-1] if len(y_pred) > 0 else None)\n",
    "            avg_values.append(sum(y_pred)/len(y_pred) if len(y_pred) > 0 else None)\n",
    "\n",
    "    # Select ground truth values for the available indices\n",
    "    y_true_selected = [y_true_full[i] for i in indices]\n",
    "\n",
    "    # Filter out indices 0 and 33\n",
    "    filtered_indices = [i for i, idx in enumerate(indices) if idx not in (0, 33)]\n",
    "    y_true_filtered = [y_true_selected[i] for i in filtered_indices]\n",
    "    first_filtered = [first_values[i] for i in filtered_indices]\n",
    "    second_filtered = [second_values[i] for i in filtered_indices]\n",
    "    last_filtered = [last_values[i] for i in filtered_indices]\n",
    "    avg_filtered = [avg_values[i] for i in filtered_indices]\n",
    "\n",
    "    # Compute metrics per folder using filtered values\n",
    "    all_metrics[\"First\"].append(compute_metrics(y_true_filtered, first_filtered))\n",
    "    all_metrics[\"Second\"].append(compute_metrics(y_true_filtered, second_filtered))\n",
    "    all_metrics[\"Last\"].append(compute_metrics(y_true_filtered, last_filtered))\n",
    "    all_metrics[\"Average\"].append(compute_metrics(y_true_filtered, avg_filtered))\n",
    "\n",
    "# Compute averages across folders\n",
    "avg_metrics_final = {key: average_metrics(all_metrics[key]) for key in all_metrics}\n",
    "\n",
    "# Display as a table\n",
    "table = []\n",
    "for key, metrics in avg_metrics_final.items():\n",
    "    table.append([\n",
    "        key, \n",
    "        round(metrics[\"MAE_avg\"], 4),\n",
    "        round(metrics[\"MAE_dev\"], 4),\n",
    "        round(metrics[\"MSE_avg\"], 4),\n",
    "        round(metrics[\"RMSE_avg\"], 4),\n",
    "        round(metrics[\"Accuracy_avg\"], 4)\n",
    "    ])\n",
    "\n",
    "headers = [\"Prediction Type\", \"MAE Avg\", \"MAE Dev\", \"MSE Avg\", \"RMSE Avg\", \"Accuracy Avg\"]\n",
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
